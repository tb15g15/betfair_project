{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Idea 2 : \"Cointegration - pairs trading\"\n",
    "\n",
    "__Section 0: Setup__ Importing packages/reading in data etc.\n",
    "\n",
    "__Section 1 : Idea__ \n",
    "\n",
    "- __1.1__ Strategy idea\n",
    "\n",
    "- __1.2__ Origin of idea. Context/Reasoning for strategy to work e.g. use in financial markets?\n",
    "\n",
    "__Section 2 : Exploration__\n",
    "\n",
    "- __2.1__ Exploratory Data Analysis. e.g plots of price/volumes that could show strategy working, how much potential.\n",
    "\n",
    "- __2.2__ Define some 'strategy metrics'. Metrics that can can you use to gauge if this strategy will work i.e no.price points above a certain threshold that is profitable. Metrics could show how often there is an opportunity to make a trade and how much 'value' is in an opportunity e.g. how much is there a price swing?\n",
    "\n",
    "\n",
    "__Section 3 : Strategy testing__\n",
    "\n",
    "- __3.1__ Testing strategy on previous data. \n",
    "\n",
    "- __3.2__ State any assumptions made by testing.\n",
    "\n",
    "- __3.3__ Model refinements. How could strategy be optimised? Careful : is this backfitting/overfitting - what measures taken to negate this e.g. bootstrapping?\n",
    "\n",
    "- __3.4__ Assessing strategy. P/L on data sample? ROI? variance in results? longest losing run?\n",
    "\n",
    "__Section 4 : Practical requirements__\n",
    "\n",
    "- __4.1__ Identify if this edge is ‘realisable’? What methods will you apply to extract this value? e.g. applying a hedge function\n",
    "\n",
    "\n",
    "- __4.2__ Is it possible to quantify the potential profit from the strategy? Consideration : How long will it take to obtain this? How 'risky' is it? e.g. if something did go wrong, how much do we lose? \n",
    "\n",
    "- __4.3__ Strategy limitations. The factors that could prevent strategy working e.g. practical considerations e.g. reacting quick enough to market updates, volume behind a price, size of bankroll needed\n",
    "\n",
    "\n",
    "__Section 5: Potential limitations__\n",
    "\n",
    "- __5.1__ What is our 'competition' - if not quantifiable, do we suspect people are doing the same thing? \n",
    "\n",
    "- __5.2__ So what's our edge? Identify ways of finding this edge in future? e.g what features are there? Are they predictive? Is there a certain 'market/runner' profile?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes (to do)\n",
    "* Use lay prices as well (currently only using back prices, but two lay bets are made per pairs trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0 : Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from pathlib import Path, PurePath \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payout(bp, bs, lp, ls, c = 0):\n",
    "    if ls == '?':\n",
    "        ls = lay_hedge_stake(bp, bs, lp, c)\n",
    "    elif bs == '?':\n",
    "        bs = bet_hedge_stake(lp, ls, bp, c)\n",
    "    loss_side = - bs + ls * (1 - c) \n",
    "    win_side = (bp - 1) * bs * (1 - c) - (lp - 1) * ls\n",
    "    return win_side, loss_side \n",
    "\n",
    "def lay_hedge_stake(bp, bs, lp, c):\n",
    "    return (((bp - 1) * bs * (1 - c)) + bs) / (lp)\n",
    "\n",
    "def bet_hedge_stake(lp, ls, bp, c):\n",
    "    return ls * (lp - c) / (bp * (1 - c) + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13073, 307)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SelectionId</th>\n",
       "      <th>MarketId</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Distance</th>\n",
       "      <th>RaceType</th>\n",
       "      <th>BSP</th>\n",
       "      <th>NoRunners</th>\n",
       "      <th>BS:T-60</th>\n",
       "      <th>BS:T-59</th>\n",
       "      <th>BS:T-58</th>\n",
       "      <th>...</th>\n",
       "      <th>LS:T+5</th>\n",
       "      <th>LS:T+6</th>\n",
       "      <th>LS:T+7</th>\n",
       "      <th>LS:T+8</th>\n",
       "      <th>LS:T+9</th>\n",
       "      <th>LS:T+10</th>\n",
       "      <th>LS:T+11</th>\n",
       "      <th>LS:T+12</th>\n",
       "      <th>LS:T+13</th>\n",
       "      <th>LS:T+14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11986132</td>\n",
       "      <td>1.169028</td>\n",
       "      <td>Huntingdon</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Chase</td>\n",
       "      <td>8.33</td>\n",
       "      <td>9</td>\n",
       "      <td>16.43</td>\n",
       "      <td>24.51</td>\n",
       "      <td>26.57</td>\n",
       "      <td>...</td>\n",
       "      <td>10.08</td>\n",
       "      <td>11.15</td>\n",
       "      <td>5.44</td>\n",
       "      <td>7.09</td>\n",
       "      <td>14.16</td>\n",
       "      <td>19.53</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16800725</td>\n",
       "      <td>1.169028</td>\n",
       "      <td>Huntingdon</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Chase</td>\n",
       "      <td>3.68</td>\n",
       "      <td>9</td>\n",
       "      <td>15.43</td>\n",
       "      <td>25.74</td>\n",
       "      <td>57.82</td>\n",
       "      <td>...</td>\n",
       "      <td>29.87</td>\n",
       "      <td>221.22</td>\n",
       "      <td>43.23</td>\n",
       "      <td>43.10</td>\n",
       "      <td>13.53</td>\n",
       "      <td>26.15</td>\n",
       "      <td>13.60</td>\n",
       "      <td>74.30</td>\n",
       "      <td>419.52</td>\n",
       "      <td>23082.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SelectionId  MarketId       Venue  Distance RaceType   BSP  NoRunners  \\\n",
       "0     11986132  1.169028  Huntingdon      20.0    Chase  8.33          9   \n",
       "1     16800725  1.169028  Huntingdon      20.0    Chase  3.68          9   \n",
       "\n",
       "   BS:T-60  BS:T-59  BS:T-58  ...  LS:T+5  LS:T+6  LS:T+7  LS:T+8  LS:T+9  \\\n",
       "0    16.43    24.51    26.57  ...   10.08   11.15    5.44    7.09   14.16   \n",
       "1    15.43    25.74    57.82  ...   29.87  221.22   43.23   43.10   13.53   \n",
       "\n",
       "   LS:T+10  LS:T+11  LS:T+12  LS:T+13   LS:T+14  \n",
       "0    19.53     3.12     3.31     0.68      0.68  \n",
       "1    26.15    13.60    74.30   419.52  23082.10  \n",
       "\n",
       "[2 rows x 307 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in data\n",
    "project_dir = Path.cwd().parents[2]\n",
    "data_dir = project_dir / 'data' / 'processed' / 'api' / 'advanced' / 'adv_data.csv'\n",
    "df = pd.read_csv(data_dir, index_col = 0)\n",
    "print(df.shape)\n",
    "\n",
    "# defining variables\n",
    "back_prices = [col for col in df.columns if 'BP' in col]\n",
    "back_sizes = [col for col in df.columns if 'BS' in col]\n",
    "lay_prices = [col for col in df.columns if 'LP' in col]\n",
    "lay_sizes = [col for col in df.columns if 'LS' in col]\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 : Pairs trading\n",
    "\n",
    "__1.1__ **- Idea**\n",
    "\n",
    "Pairs trading is a statistical arbitrage strategy whereby the trader attempts to exploit the tendency of some pairs of assets to have similar price changes. For example, consider two telecoms companies of similar size and profitability who are exposed to the same market conditions. Investors/traders see this similarity, and as such their prices may react in the same way to news. If there is a quantifiable relationship where the prices have a 'spread' of X on average, if the shares deviate from this average such that one is undervalued and one is overvalued, a long-short strategy can profit from a return to the average.\n",
    "\n",
    "[Bebbington, PA (2017)](https://discovery.ucl.ac.uk/id/eprint/1563501/) looks at pairs trading in horse racing markets. \n",
    "\n",
    "The following outlines Bebbington's method for analysing this strategy.\n",
    "\n",
    "* The 'signals' are the best back or lay price available at a given timestamp.\n",
    "* A z-score transformation of the log of the decimal odds is used to standardise prices. This makes the relative directional movement in different prices comparable by accounting for their respective variances. \n",
    "* Non-overlapping windows of data, for example, price observations 1-5, 6-10, 11-15, are analysed to find pairs of horses. \n",
    "* Pairs are discovered by analysing the sum of squared distances between two horses' prices throughout time. Those that move the least relative to each other are the best candidates for pairs.\n",
    "* Once pairs are identified, the 'spread' between their prices (on average, or at the end of each window) is compared to a minimum size requirement for a bet to be made, $\\phi$.\n",
    "* The 'hedging ratio' is found using an OLS regression of the price of one of the horses on the price of the other. Since the two prices are pairs but will have different variances and absolute values, their movement relative to eachother must be considered to make the strategy 'cost neutral'. It is also used to define the stake size. \n",
    "* The final observed spread indicates which on which horse a 'back-to-lay' hedge must be made (that which is expected to be backed in) and on which a 'lay-to-back' hedge must be made (that which is expected to drift). This spread is compared to an interval [?], likely a confidence interval of past spreads or simply the interval of observed spreads. If the spread is greater than usual or smaller than usual, the bets are placed. \n",
    "* Trades occur at the end of each window of data with a 5 second delay. This is used to simulate a method where the algorithm is reacting to live data. In our analysis, we look at prices for the first 30 price points and make bets in the remaining 30 periods.\n",
    "* In the paper it appears that both sides of the hedge bet are made at the same point in time.\n",
    "\n",
    "These steps are attempted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2__ **- Setup**\n",
    "\n",
    "**Data**\n",
    "\n",
    "The following example will be set up with a random race and will identify tradeable pairs (or that there are none). Three DataFrames are created: (1) the unchanged race sample DataFrame with one row per horse and data going along in columns, (2) a back prices DataFrame with one column per horse and prices going through time in rows, (3) the same for lay prices. This analysis looks at prices before the race begins.\n",
    "\n",
    "There are 60 price data points for each horse, finishing at the begining of the race.\n",
    "\n",
    "Variables:\n",
    "* $BP_{t}^{i}$ is back price for horse i at time t.\n",
    "* $LP_{t}^{i}$ is lay price for horse i at time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h10163711</th>\n",
       "      <th>h16117335</th>\n",
       "      <th>h20179313</th>\n",
       "      <th>h21104332</th>\n",
       "      <th>h22403664</th>\n",
       "      <th>h26641007</th>\n",
       "      <th>h27535763</th>\n",
       "      <th>h4003026</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.50</td>\n",
       "      <td>8.0</td>\n",
       "      <td>46.43</td>\n",
       "      <td>16.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>190.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.61</td>\n",
       "      <td>7.8</td>\n",
       "      <td>48.00</td>\n",
       "      <td>16.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>190.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h10163711  h16117335  h20179313  h21104332  h22403664  h26641007  \\\n",
       "0      19.50        8.0      46.43       16.5       12.0       1.69   \n",
       "1      19.61        7.8      48.00       16.5       12.0       1.69   \n",
       "\n",
       "   h27535763  h4003026  \n",
       "0      190.0      14.0  \n",
       "1      190.0      14.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = df[df['MarketId'] == df['MarketId'].sample(1).item()]\n",
    "\n",
    "bp_df = sample_df[['SelectionId'] + back_prices].copy()\n",
    "new_cols = bp_df.columns.str.replace(\"[BP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "bp_df.rename(columns = dict(zip(bp_df.columns, new_cols)), inplace = True)\n",
    "bp_t_df = bp_df.T.copy()\n",
    "bp_t_df.columns = [\"h\" + str(int(column)) for column in bp_t_df.iloc[0]]\n",
    "bp_t_df = bp_t_df.iloc[1:-15] # using the 60 pre-off price data points\n",
    "bp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "lp_df = sample_df[['SelectionId'] + lay_prices].copy()\n",
    "new_cols = lp_df.columns.str.replace(\"[LP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "lp_df.rename(columns = dict(zip(lp_df.columns, new_cols)), inplace = True)\n",
    "lp_t_df = lp_df.T.copy()\n",
    "lp_t_df.columns = [\"h\" + str(int(column)) for column in lp_t_df.iloc[0]] #rename columns to horse ids\n",
    "lp_t_df = lp_t_df.iloc[1:-15] #remove horse ids, remove inplay data\n",
    "lp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# bsp_df = sample_df[['BSP']].copy()\n",
    "# bsp_df['min_bp'] = sample_df['BSP'].apply(lambda x: round(utils.back_hedge_min_bp(x, 0.05), 2))\n",
    "# bsp_df['max_lp'] = sample_df['BSP'].apply(lambda x: round(utils.lay_hedge_max_lp(x, 0.05), 2))    \n",
    "\n",
    "bp_t_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.3__ **- Z-score transformation**\n",
    "\n",
    "Bebbington standardises prices by taking the natural logarithm of each price and then standardising it with a Z-score transformation. \n",
    "\n",
    "Taking $P_{t}^{i} = ln(BP_{t}^{i})$, this means finding \n",
    "\n",
    "#### $P_{t}^{'(i)} = \\frac{P_{t}^{i}-\\overline{P}_{t}^{i}}{\\sigma^{(i)}}$\n",
    "\n",
    "where $\\sigma^{(i)}$ is the standard deviation of the horse's price throughout the time series.\n",
    "\n",
    "The Z-score transformation gives the relationship between an individual data point in the sample relative to that of the population mean and standard deviation. This means that variations are comparable between horses.\n",
    "\n",
    "The following will look at the first 30 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h10163711</th>\n",
       "      <th>h16117335</th>\n",
       "      <th>h20179313</th>\n",
       "      <th>h21104332</th>\n",
       "      <th>h22403664</th>\n",
       "      <th>h26641007</th>\n",
       "      <th>h27535763</th>\n",
       "      <th>h4003026</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.451617</td>\n",
       "      <td>2.101047</td>\n",
       "      <td>-0.635630</td>\n",
       "      <td>1.153069</td>\n",
       "      <td>-2.051518</td>\n",
       "      <td>-0.280802</td>\n",
       "      <td>1.148649</td>\n",
       "      <td>1.528065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.606916</td>\n",
       "      <td>1.363358</td>\n",
       "      <td>0.215995</td>\n",
       "      <td>1.153069</td>\n",
       "      <td>-2.051518</td>\n",
       "      <td>-0.280802</td>\n",
       "      <td>1.148649</td>\n",
       "      <td>1.528065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h10163711  h16117335  h20179313  h21104332  h22403664  h26641007  \\\n",
       "0   1.451617   2.101047  -0.635630   1.153069  -2.051518  -0.280802   \n",
       "1   1.606916   1.363358   0.215995   1.153069  -2.051518  -0.280802   \n",
       "\n",
       "   h27535763  h4003026  \n",
       "0   1.148649  1.528065  \n",
       "1   1.148649  1.528065  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_bp_df = bp_t_df.copy()\n",
    "z_bp_df = z_bp_df[:30] #first 30 observations\n",
    "z_bp_df = np.log(z_bp_df)\n",
    "\n",
    "for column in z_bp_df.columns:\n",
    "    mean = z_bp_df[column].mean()\n",
    "    sd = np.std(z_bp_df[column], ddof = 1)\n",
    "    z_bp_df[column] = z_bp_df[column].apply(lambda x: (x - mean) / sd)\n",
    "    \n",
    "z_bp_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.4__ **- Sum of squared distances**\n",
    "\n",
    "Following Bebbington, to select pairs, we create a matrix (DataFrame, in this case) of the sum of squared distances between pairs of horses throughout the time series.\n",
    "\n",
    "$\\Theta _{ij} = \\left\\{\\begin{matrix}\n",
    "\\sum_{M}^{t=1}(P_{t}^{'(i)} - P_{t}^{'(j)})^{2}, &i\\neq j\\\\ \n",
    " 0, i=j& \n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h10163711</th>\n",
       "      <th>h16117335</th>\n",
       "      <th>h20179313</th>\n",
       "      <th>h21104332</th>\n",
       "      <th>h22403664</th>\n",
       "      <th>h26641007</th>\n",
       "      <th>h27535763</th>\n",
       "      <th>h4003026</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horse</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h10163711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h16117335</th>\n",
       "      <td>28.233027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h20179313</th>\n",
       "      <td>61.114044</td>\n",
       "      <td>32.663370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h21104332</th>\n",
       "      <td>46.560153</td>\n",
       "      <td>34.091779</td>\n",
       "      <td>68.108368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h22403664</th>\n",
       "      <td>103.124656</td>\n",
       "      <td>105.536149</td>\n",
       "      <td>63.222708</td>\n",
       "      <td>84.208128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h26641007</th>\n",
       "      <td>69.778177</td>\n",
       "      <td>101.531219</td>\n",
       "      <td>86.147787</td>\n",
       "      <td>70.059679</td>\n",
       "      <td>37.405348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h27535763</th>\n",
       "      <td>42.675603</td>\n",
       "      <td>55.851381</td>\n",
       "      <td>65.060994</td>\n",
       "      <td>60.074608</td>\n",
       "      <td>56.933814</td>\n",
       "      <td>63.700918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h4003026</th>\n",
       "      <td>48.117953</td>\n",
       "      <td>25.424362</td>\n",
       "      <td>68.149755</td>\n",
       "      <td>40.541261</td>\n",
       "      <td>86.148868</td>\n",
       "      <td>84.084856</td>\n",
       "      <td>53.016157</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            h10163711   h16117335  h20179313  h21104332  h22403664  h26641007  \\\n",
       "horse                                                                           \n",
       "h10163711         NaN         NaN        NaN        NaN        NaN        NaN   \n",
       "h16117335   28.233027         NaN        NaN        NaN        NaN        NaN   \n",
       "h20179313   61.114044   32.663370        NaN        NaN        NaN        NaN   \n",
       "h21104332   46.560153   34.091779  68.108368        NaN        NaN        NaN   \n",
       "h22403664  103.124656  105.536149  63.222708  84.208128        NaN        NaN   \n",
       "h26641007   69.778177  101.531219  86.147787  70.059679  37.405348        NaN   \n",
       "h27535763   42.675603   55.851381  65.060994  60.074608  56.933814  63.700918   \n",
       "h4003026    48.117953   25.424362  68.149755  40.541261  86.148868  84.084856   \n",
       "\n",
       "           h27535763  h4003026  \n",
       "horse                           \n",
       "h10163711        NaN       NaN  \n",
       "h16117335        NaN       NaN  \n",
       "h20179313        NaN       NaN  \n",
       "h21104332        NaN       NaN  \n",
       "h22403664        NaN       NaN  \n",
       "h26641007        NaN       NaN  \n",
       "h27535763        NaN       NaN  \n",
       "h4003026   53.016157       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [column for column in z_bp_df.columns]\n",
    "\n",
    "matrix = z_bp_df.iloc[0:0].copy()\n",
    "\n",
    "matrix.insert(0, \"horse\", np.array(ids))\n",
    "matrix = matrix.set_index(\"horse\", drop = False)\n",
    "del matrix[\"horse\"]\n",
    "\n",
    "for column in matrix.columns:\n",
    "    for row in matrix.index:\n",
    "        if column == row:\n",
    "            matrix.loc[row, column] = np.nan\n",
    "        else: \n",
    "            matrix.loc[row, column] = ((z_bp_df[row] - z_bp_df[column]) ** 2).sum() or np.nan\n",
    "            \n",
    "for x in range(len(ids)):\n",
    "    for y in range(x, len(ids)):\n",
    "        matrix.iloc[x, y] = np.nan\n",
    "        \n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair found: horse h4003026 and horse h16117335 with sum of squared spreads equal to 25.42436206727571.\n"
     ]
    }
   ],
   "source": [
    "horse_x = matrix.min(axis=1).idxmin()\n",
    "horse_y = matrix.min().idxmin()\n",
    "sss = matrix.min().min()\n",
    "\n",
    "print(f\"Pair found: horse {horse_x} and horse {horse_y} with sum of squared spreads equal to {sss}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.5__ **- Regression of prices of horse Y on horse X**\n",
    "\n",
    "With $x_{t} = \\left \\{P_{1}^{X} + P_{2}^{X} + , ... , P_{M}^{X} \\right \\}$ and  $y_{t} = \\left \\{P_{1}^{Y} + P_{2}^{Y} + , ... , P_{M}^{Y} \\right \\}$ where $M$ is the final time period in the window, we carry out the OLS regression of $y_{t}$ on $x_{t}$. The estimate of $\\beta$ is the hedging ratio, giving the relative holding of $x_{t}$ for a cost-neutral hedge position.\n",
    "\n",
    "$y_{t} = \\beta x_{t} + \\varepsilon_{t}$\n",
    "\n",
    "Using this estimation and the final end of window observations at time $M$ we get the spread at the end of the window: $\\varepsilon_{M} = y_{M} - \\hat{\\beta} x_{M}$. If $\\varepsilon_{M}$ is outside of an interval of past spread values such that if the spread returns to the mean a hedge bet will be profitable, bets can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final period estimated spread (in log prices) epsilon = -0.07810240582111483.\n"
     ]
    }
   ],
   "source": [
    "#regression setup\n",
    "reg_df = bp_t_df[[horse_y, horse_x]][:30].copy() #non-standardised prices\n",
    "reg_df = np.log(reg_df) \n",
    "reg_df['const'] = 1\n",
    "\n",
    "#regression fit and results\n",
    "reg = sm.OLS(endog=reg_df[horse_y], exog=reg_df[['const', horse_x]], missing='drop')\n",
    "\n",
    "results = reg.fit()\n",
    "\n",
    "constant = results.params[0]\n",
    "beta = results.params[1]\n",
    "\n",
    "#estimated final period (T=30) spread\n",
    "spread = reg_df[horse_y].iloc[29].item() - constant - beta * reg_df[horse_x].iloc[29].item()\n",
    "\n",
    "print(f\"Final period estimated spread (in log prices) epsilon = {spread}.\")\n",
    "\n",
    "#Positive spread: horse Y has drifted from the mean, horse X has been backed in. If mean reversion occurs horse Y will be backed in and horse X will drift. \n",
    "# -> Back-to-lay hedge Y and lay-to-back hedge X\n",
    "#Negative spread: horse X has drifted from the mean, horse Y has been backed in. If mean reversion occurs horse X will be backed in and horse Y will drift. \n",
    "# -> Back-to-lay hedge X and lay-to-back hedge Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.6.1__ **- Pairs trade**\n",
    "\n",
    "Bebbing describes the following steps for the pairs trade:\n",
    "\n",
    "If the final estimated residual $\\varepsilon_{M}$ is:\n",
    "* Less than the lower bound of the threshold: back horse Y with £1, lay horse X with £$\\beta$.\n",
    "* Greater than the upper bound of the threshold: lay horse Y with £1 and back horse X with £$\\beta$.\n",
    "\n",
    "Bebbington describes using a self-imposed delay of 5 seconds to emulate the delays in placing a bet on the Betfair Exchange. Since each time step in this dataset before the beginning of the race is 2 minutes, 1 step will be used. The prices at time 31 (index = 30) will be used.\n",
    "\n",
    "Put alone, this strategy doesnt make sense. The bettor is still exposed to the result of the race without closing out each position with an opposing bet on each horse after some period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.6.2__ **- Pairs trade 2**\n",
    "\n",
    "In a pairs trade with two financial assets (say, company shares) the objective is to go long on the share whose price is expected to increase, while shorting the share whose price is expected to decrease. In both cases the positions would be closed out: the asset is sold or bought back (in the case of the short-sale) once there is a gain or possibly after a set interval. In the prior example, the closing out of each position should be done via an opposite back/lay.\n",
    "\n",
    "If the final estimated residual $\\varepsilon_{M}$ is:\n",
    "* Less than the lower bound of the threshold: back horse Y with £1, lay horse X with £$\\beta$. **After k periods, lay horse Y and back horse X with the optimal stakes given the prevailing prices, as defined in utils.py.**\n",
    "* Greater than the upper bound of the threshold: lay horse Y with £1 and back horse X with £$\\beta$. **After k periods, back horse Y and lay horse X with the optimal stakes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 35\n",
    "\n",
    "# if spread < 0:\n",
    "#     #back to lay Y\n",
    "#     bp_y = bp_t_df[horse_y].iloc[30]\n",
    "#     lp_y = bp_t_df[horse_y].iloc[k]\n",
    "    \n",
    "#     win_side_y, loss_side_y = utils.payout(bp_y, 1, lp_y, '?')\n",
    "#     print(f\"Win side = {win_side_y}. Loss side = {loss_side_y}.\")\n",
    "    \n",
    "#     #lay to back X\n",
    "#     lp_x = bp_t_df[horse_x].iloc[30]\n",
    "#     bp_x = bp_t_df[horse_x].iloc[k]\n",
    "    \n",
    "#     win_side_x, loss_side_x = utils.payout(bp_x, '?', lp_x, beta)\n",
    "#     print(f\"Win side = {win_side_x}. Loss side = {loss_side_x}.\")\n",
    "        \n",
    "# else:\n",
    "#     #lay to back Y\n",
    "#     lp_y = bp_t_df[horse_y].iloc[30]\n",
    "#     bp_y = bp_t_df[horse_y].iloc[k]\n",
    "    \n",
    "#     win_side_y, loss_side_y = utils.payout(bp_y, '?', lp_y, 1)\n",
    "#     print(f\"Win side = {win_side_y}. Loss side = {loss_side_y}.\")\n",
    "    \n",
    "#     #back to lay X\n",
    "#     bp_x = bp_t_df[horse_x].iloc[30]\n",
    "#     lp_x = bp_t_df[horse_x].iloc[k]\n",
    "    \n",
    "#     win_side_x, loss_side_x = utils.payout(bp_x, beta, lp_x, '?')\n",
    "#     print(f\"Win side = {win_side_x}. Loss side = {loss_side_x}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe Bebbington's methodology has flaws. The following attempts to use a more robust method for pairs trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach to pairs trading\n",
    "\n",
    "__2.0__ **- [Herlemont (2004)](http://docs.finance.free.fr/DOCS/Yats/cointegration-en%5B1%5D.pdf) paper**\n",
    "\n",
    "Herlemont describes in detail the econometrics of pairs trading for financial market assets. The following partly follows his commentary with some additional clarifications and discussion relating to horse racing.\n",
    "\n",
    "**2.1 - Testing for mean reversion**\n",
    "\n",
    "The aim is to identify odds that move together and whose spread is mean reverting. For the purposes of horse racing pairs, mean reversion is essential. Our objective is to capture prices whose spread has (temporarily) deviated from its mean. If this can be found, bets can be made to take advantage of the possible reversion.\n",
    "\n",
    "A stochastic process $y_{t}$ that is weakly stationary has the following properties for all $t$:\n",
    "\n",
    "* $E[y_{t}] = \\mu < \\infty$\n",
    "* $var(y_{t}) = \\gamma_{0} < \\infty$\n",
    "* $cov(y_{t}, y_{t-j}) = \\gamma_{j} < \\infty, j = 1, 2, 3 ...$\n",
    "\n",
    "(constant mean, constant variance, covariance between two observations depends only on the distance in time between them)\n",
    "\n",
    "A weakly stationary $I(0)$ series:\n",
    "* Fluctuates around its mean with a finite variance that does not depend upon time.\n",
    "* Is mean-reverting: it has tendency to return to its mean.\n",
    "* Has limited memory; the effect of a shock dies out. Autocorrelations die out (fairly) rapidly.\n",
    "\n",
    "With two horse's odds, $A_{t}$ and $B_{t}$, we look at $y_{t} = \\log \\frac{A_{t}}{B_{t}} = \\log A_{t} - \\log B_{t}$. This is once again the spread between the prices of the two horses, defined slightly differently. We want to find a pair which has a weakly stationary spread. We are interested in the ($AR(1)$) process \n",
    "\n",
    "$y_{t} = c + \\theta y_{t-1} + \\varepsilon_{t}$,\n",
    "\n",
    "or the log odds ratio over time. If this is weakly stationary, it would suggest a mean reverting process. \n",
    "\n",
    "The three previous conditions, and a stability condition that $|\\theta|<1$ (that the process $y_{t}$ is not a random walk or that it follows an eratic positive-to-negative pattern) must hold.\n",
    "______\n",
    "\n",
    "A Dickey-Fuller stationarity test can be carried out on the log ratio of the prices to test whether a process is weakly stationary. If we carry out the regression:\n",
    "\n",
    "$\\Delta y_{t} = \\mu + \\omega y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "where the null hypothesis that $\\omega = 0$ is that the 'true' relationship is $\\Delta y_{t} = \\mu + \\varepsilon_{t} \\Leftrightarrow y_{t} = \\mu + y_{t-1} + \\varepsilon_{t}$, or a random walk with starting point $y_{0} = \\mu$.\n",
    "\n",
    "If we can reject the null hypothesis, the price ratio is weakly stationary and thereby mean-reverting.\n",
    "\n",
    "A Dickey-Fuller test is required for each possible pair of horses in a race, or $\\frac{n(n-1)}{2}$ regressions, where $n$ is the number of horses.\n",
    "\n",
    "While we are interested in the stochastic process $y_{t}$, we do not need to carry out the regression of $y_{t} = c + \\theta y_{t-1} + \\varepsilon_{t}$ for the purpose of finding pairs. This relationship between a pair of odds itself is not important to quantify. We are only interested in the features of the process. \n",
    "____\n",
    "\n",
    "*In the previous analysis, the test for whether two odds formed a pair was to find the pair with the smallest sum of absolute differences over time in the standardised prices. That method would allow maximum 1 pair to be found per race, and the validity of that pair would not be confirmed statisticallyather. Rather, the pair's feasibilty for a trade would be tested for afterwards based on profitability. I have more confidence in the approach in this section.*\n",
    "\n",
    "**2.2 - Screening pairs**\n",
    "\n",
    "Herlemont describes rules to ensure that market neutrality is more achievable in pairs trading. The idea is to pick stocks with very similar characteristics like same industry and similar market betas, with the intention of minimising asymmetric shocks to the price of one stock and not the other. For example in the case of two stocks, the share on which you are long is a business heavily dependent on oil, while the other share is not, a surge in oil prices which dampens profitability of your long share will likely see its price fall, ruining the pairs trade. In the case of shares, the simplest solution would be to pick shares in similar industries with similar market betas (or with similar idiosyncratic risks).\n",
    "\n",
    "For horses, the external factors influencing prices (news about runners, changing weather conditions, etc.) will usually always have asymmetric effects. This may be avoidable through picking horses with similar fundamental characteristics. However, this is very complicated. My hope is that the pair finding mechanism picks horses where this is already the case, because the market reacts the same way to news for these horse pairs.\n",
    "\n",
    "We cannot follow a beta-based approach because there are not 'market-wide fluctuations' of the same sort. However, there is the fact that the implied probability of all horses in the market book is equal to approximately 1. Therefore, you could say that for a given change in implied probability for one horse, the sum of the changes in the odds of all the remaining horses is the negative the change for the given horse:\n",
    "\n",
    "$\\Delta O_{i} = - \\sum_{j = 1, j \\neq i}^{N_{h}} \\Delta O_{j} $\n",
    "\n",
    "There is therefore interdependence between all prices across the market. It's possible that this will cause an endogeneity problem in regressions between separate horses, as the changes in the dependent variable necessarily impact the explanatory variable. However, the impact is likely to be very small, and will be smaller the greater the number of horses. \n",
    "\n",
    "*In Bebbington's analysis, he describes that betting £1 on one of the horses and £$\\beta$ on the other creates a market neutral bet. This is incorrect, and it appears that he has misunderstood hedging in this context. In that analysis, $\\beta = \\frac{y_{t}}{x_{t}}$, and therefore he is simply considering the ratio of the prices of the horses, the same ratio considered when determining the optimal stake for two given prices in a hedge. It is correct that on a single horse this creates a market neutral bet, however neutrality in horse racing means neutral to the outcome of the race. Any bet neutral to the race outcome is definitively neutral to the market. When betting on separate horses, the bets on each horse must be made neutral separately. Additionally, the use of $\\beta$ in staking is unneccesary. Consider the case where £$BS$ has been bet on horse A at price $BP$. Now, horse A is priced at $LP$. The optimal stake to bet on LP is £$LS = \\frac{BS * BP}{LP}$. In the aforementioned regression, $BS = 1$, hence $\\beta = \\frac{y_{t} * 1}{x_{t}}$ is the optimal stake only for bets of £1, otherwise it would be $S*\\beta$. More importantly, using the estimated $\\beta$ to find the an approximation of the optimal stake makes no sense when you can simply find the optimal stake with the aforementioned equation.*\n",
    "\n",
    "**2.3 - Trading rules**\n",
    "\n",
    "Timing rules must be added. \n",
    "\n",
    "Herlemont's basic rule is \"to open a position when the ratio of two share prices hits the 2 rolling standard deviation [difference from the 130-day rolling mean] and close it when the ratio returns to the mean.\"\n",
    "\n",
    "To avoid opening a position on stocks that are deviating from the mean and are going to deviate further, Herlemont describes that \"the position is not opened when the ratio breaks the two-standard-deviations limit for the first time, but rather when it crosses it to revert to the mean again.\"\n",
    "\n",
    "This can be achieved with the horse odds, of course in far smaller time scales. The current dataset is in 5-minute intervals for the three hours before a race; this should likely be expanded.\n",
    "\n",
    "Stop losses should be included and trade length should also be limited.\n",
    "\n",
    "Rules:\n",
    "1. Trade on pairs whose spread is reapproaching the mean from a deviated position\n",
    "2. Stop loss at x% of the initial position\n",
    "3. Don't hold open pairs trades for longer than x hours. \n",
    "\n",
    "It should be possible to quantify the average length of time required for a mean reversion and therefore the maximum logical time to hold open a position by looking at past data.\n",
    "\n",
    "**2.4 - Other tests and considerations**\n",
    "\n",
    "1. It should be ensured that the regression results of one price on another are not spurious (as with the regression in 2.5). $\\beta$ could be statistically meaningless if it is, meaning that it makes no sense to use it.\n",
    "2. I will also test whether $y_{t} = c + \\theta y_{t-1} + \\varepsilon_{t}$ is $I(1)$, or difference stationary. If we can rule this out, this gives more confidence in the 'weak-stationarity' of the spread over time.\n",
    "3. I will look out for $\\omega$ in the DF test that are close to 1 yet pass the DF test. They will have lots of features of a random walk, so the pairs exercise might be meaningless.\n",
    "4. Structural breaks (in this case, large instantaneous jumps in the spread) may make series that are stationary on either side of the break appear non-stationary. This is hard to account for in testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#new sample\n",
    "sample_df = df[df['MarketId'] == df['MarketId'].sample(1).item()]\n",
    "sample_df.drop_duplicates(inplace=True)\n",
    "\n",
    "bp_df = sample_df[['SelectionId'] + back_prices].copy()\n",
    "new_cols = bp_df.columns.str.replace(\"[BP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "bp_df.rename(columns = dict(zip(bp_df.columns, new_cols)), inplace = True)\n",
    "bp_t_df = bp_df.T.copy()\n",
    "bp_t_df.columns = [\"h\" + str(int(column)) for column in bp_t_df.iloc[0]]\n",
    "bp_t_df = bp_t_df.iloc[1:-15] # using the 60 pre-off price data points\n",
    "bp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "lp_df = sample_df[['SelectionId'] + lay_prices].copy()\n",
    "new_cols = lp_df.columns.str.replace(\"[LP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "lp_df.rename(columns = dict(zip(lp_df.columns, new_cols)), inplace = True)\n",
    "lp_t_df = lp_df.T.copy()\n",
    "lp_t_df.columns = [\"h\" + str(int(column)) for column in lp_t_df.iloc[0]] #rename columns to horse ids\n",
    "lp_t_df = lp_t_df.iloc[1:-15] #remove horse ids, remove inplay data\n",
    "lp_t_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h13478734</th>\n",
       "      <th>h17247906</th>\n",
       "      <th>h19148736</th>\n",
       "      <th>h19252818</th>\n",
       "      <th>h19641188</th>\n",
       "      <th>h22121327</th>\n",
       "      <th>h24872</th>\n",
       "      <th>h26698821</th>\n",
       "      <th>h8319283</th>\n",
       "      <th>h8536472</th>\n",
       "      <th>h9659886</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.449279</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>2.136531</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>1.593309</td>\n",
       "      <td>1.824549</td>\n",
       "      <td>3.216874</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>3.248823</td>\n",
       "      <td>4.084294</td>\n",
       "      <td>2.703373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484907</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>2.128232</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.822935</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>4.096176</td>\n",
       "      <td>3.242202</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>2.674149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.484907</td>\n",
       "      <td>3.144583</td>\n",
       "      <td>2.124654</td>\n",
       "      <td>1.460938</td>\n",
       "      <td>1.593309</td>\n",
       "      <td>1.824549</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>4.077198</td>\n",
       "      <td>3.249599</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>2.674149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.484907</td>\n",
       "      <td>3.181797</td>\n",
       "      <td>2.112635</td>\n",
       "      <td>1.479329</td>\n",
       "      <td>1.570697</td>\n",
       "      <td>1.819699</td>\n",
       "      <td>3.213260</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>3.287655</td>\n",
       "      <td>4.181897</td>\n",
       "      <td>2.674149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.484907</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>2.104134</td>\n",
       "      <td>1.474763</td>\n",
       "      <td>1.581038</td>\n",
       "      <td>1.803359</td>\n",
       "      <td>3.203965</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>3.266522</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>2.667228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h13478734  h17247906  h19148736  h19252818  h19641188  h22121327    h24872  \\\n",
       "0   2.449279   3.091042   2.136531   1.481605   1.593309   1.824549  3.216874   \n",
       "1   2.484907   3.091042   2.128232   1.481605   1.609438   1.822935  3.218876   \n",
       "2   2.484907   3.144583   2.124654   1.460938   1.593309   1.824549  3.218876   \n",
       "3   2.484907   3.181797   2.112635   1.479329   1.570697   1.819699  3.213260   \n",
       "4   2.484907   3.258097   2.104134   1.474763   1.581038   1.803359  3.203965   \n",
       "\n",
       "   h26698821  h8319283  h8536472  h9659886  \n",
       "0   4.094345  3.248823  4.084294  2.703373  \n",
       "1   4.096176  3.242202  4.094345  2.674149  \n",
       "2   4.077198  3.249599  4.174387  2.674149  \n",
       "3   4.007333  3.287655  4.181897  2.674149  \n",
       "4   4.007333  3.266522  4.248495  2.667228  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.1 - testing for mean reversion\n",
    "\n",
    "#using non-standardised log price data\n",
    "log_bp = np.log(bp_t_df[:30]).copy()\n",
    "log_bp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h13478734_h17247906</th>\n",
       "      <th>h13478734_h19148736</th>\n",
       "      <th>h13478734_h19252818</th>\n",
       "      <th>h13478734_h19641188</th>\n",
       "      <th>h13478734_h22121327</th>\n",
       "      <th>h13478734_h24872</th>\n",
       "      <th>h13478734_h26698821</th>\n",
       "      <th>h13478734_h8319283</th>\n",
       "      <th>h13478734_h8536472</th>\n",
       "      <th>h13478734_h9659886</th>\n",
       "      <th>...</th>\n",
       "      <th>h24872_h8319283</th>\n",
       "      <th>h24872_h8536472</th>\n",
       "      <th>h24872_h9659886</th>\n",
       "      <th>h26698821_h8319283</th>\n",
       "      <th>h26698821_h8536472</th>\n",
       "      <th>h26698821_h9659886</th>\n",
       "      <th>h8319283_h8536472</th>\n",
       "      <th>h8319283_h9659886</th>\n",
       "      <th>h8536472_h9659886</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.641763</td>\n",
       "      <td>0.312749</td>\n",
       "      <td>0.967675</td>\n",
       "      <td>0.855971</td>\n",
       "      <td>0.624730</td>\n",
       "      <td>-0.767594</td>\n",
       "      <td>-1.645065</td>\n",
       "      <td>-0.799543</td>\n",
       "      <td>-1.635015</td>\n",
       "      <td>-0.254093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031949</td>\n",
       "      <td>-0.867420</td>\n",
       "      <td>0.513501</td>\n",
       "      <td>0.845522</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>1.390972</td>\n",
       "      <td>-0.835471</td>\n",
       "      <td>0.545450</td>\n",
       "      <td>1.380922</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.606136</td>\n",
       "      <td>0.356675</td>\n",
       "      <td>1.003302</td>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>-0.733969</td>\n",
       "      <td>-1.611270</td>\n",
       "      <td>-0.757295</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.189242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023326</td>\n",
       "      <td>-0.875469</td>\n",
       "      <td>0.544727</td>\n",
       "      <td>0.853975</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>1.422028</td>\n",
       "      <td>-0.852143</td>\n",
       "      <td>0.568053</td>\n",
       "      <td>1.420196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.659677</td>\n",
       "      <td>0.360253</td>\n",
       "      <td>1.023969</td>\n",
       "      <td>0.891598</td>\n",
       "      <td>0.660357</td>\n",
       "      <td>-0.733969</td>\n",
       "      <td>-1.592292</td>\n",
       "      <td>-0.764692</td>\n",
       "      <td>-1.689481</td>\n",
       "      <td>-0.189242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>-0.955511</td>\n",
       "      <td>0.544727</td>\n",
       "      <td>0.827599</td>\n",
       "      <td>-0.097189</td>\n",
       "      <td>1.403050</td>\n",
       "      <td>-0.924788</td>\n",
       "      <td>0.575450</td>\n",
       "      <td>1.500239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.696890</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>1.005577</td>\n",
       "      <td>0.914210</td>\n",
       "      <td>0.665208</td>\n",
       "      <td>-0.728353</td>\n",
       "      <td>-1.522427</td>\n",
       "      <td>-0.802749</td>\n",
       "      <td>-1.696991</td>\n",
       "      <td>-0.189242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074395</td>\n",
       "      <td>-0.968637</td>\n",
       "      <td>0.539111</td>\n",
       "      <td>0.719678</td>\n",
       "      <td>-0.174564</td>\n",
       "      <td>1.333185</td>\n",
       "      <td>-0.894242</td>\n",
       "      <td>0.613507</td>\n",
       "      <td>1.507749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.773190</td>\n",
       "      <td>0.380772</td>\n",
       "      <td>1.010144</td>\n",
       "      <td>0.903868</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>-0.719059</td>\n",
       "      <td>-1.522427</td>\n",
       "      <td>-0.781616</td>\n",
       "      <td>-1.763589</td>\n",
       "      <td>-0.182322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062557</td>\n",
       "      <td>-1.044530</td>\n",
       "      <td>0.536737</td>\n",
       "      <td>0.740811</td>\n",
       "      <td>-0.241162</td>\n",
       "      <td>1.340105</td>\n",
       "      <td>-0.981973</td>\n",
       "      <td>0.599294</td>\n",
       "      <td>1.581267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   h13478734_h17247906  h13478734_h19148736  h13478734_h19252818  \\\n",
       "0            -0.641763             0.312749             0.967675   \n",
       "1            -0.606136             0.356675             1.003302   \n",
       "2            -0.659677             0.360253             1.023969   \n",
       "3            -0.696890             0.372272             1.005577   \n",
       "4            -0.773190             0.380772             1.010144   \n",
       "\n",
       "   h13478734_h19641188  h13478734_h22121327  h13478734_h24872  \\\n",
       "0             0.855971             0.624730         -0.767594   \n",
       "1             0.875469             0.661972         -0.733969   \n",
       "2             0.891598             0.660357         -0.733969   \n",
       "3             0.914210             0.665208         -0.728353   \n",
       "4             0.903868             0.681548         -0.719059   \n",
       "\n",
       "   h13478734_h26698821  h13478734_h8319283  h13478734_h8536472  \\\n",
       "0            -1.645065           -0.799543           -1.635015   \n",
       "1            -1.611270           -0.757295           -1.609438   \n",
       "2            -1.592292           -0.764692           -1.689481   \n",
       "3            -1.522427           -0.802749           -1.696991   \n",
       "4            -1.522427           -0.781616           -1.763589   \n",
       "\n",
       "   h13478734_h9659886  ...  h24872_h8319283  h24872_h8536472  h24872_h9659886  \\\n",
       "0           -0.254093  ...        -0.031949        -0.867420         0.513501   \n",
       "1           -0.189242  ...        -0.023326        -0.875469         0.544727   \n",
       "2           -0.189242  ...        -0.030723        -0.955511         0.544727   \n",
       "3           -0.189242  ...        -0.074395        -0.968637         0.539111   \n",
       "4           -0.182322  ...        -0.062557        -1.044530         0.536737   \n",
       "\n",
       "   h26698821_h8319283  h26698821_h8536472  h26698821_h9659886  \\\n",
       "0            0.845522            0.010050            1.390972   \n",
       "1            0.853975            0.001832            1.422028   \n",
       "2            0.827599           -0.097189            1.403050   \n",
       "3            0.719678           -0.174564            1.333185   \n",
       "4            0.740811           -0.241162            1.340105   \n",
       "\n",
       "   h8319283_h8536472  h8319283_h9659886  h8536472_h9659886  const  \n",
       "0          -0.835471           0.545450           1.380922      1  \n",
       "1          -0.852143           0.568053           1.420196      1  \n",
       "2          -0.924788           0.575450           1.500239      1  \n",
       "3          -0.894242           0.613507           1.507749      1  \n",
       "4          -0.981973           0.599294           1.581267      1  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe where each column is log(horse a's prices) - log(horse b's prices). one new column for all n(n-1)/2 possible pairs\n",
    "\n",
    "#use itertools to find all possible comination pairs\n",
    "combos = list(itertools.combinations(log_bp.columns, 2))\n",
    "\n",
    "#creating dataframe\n",
    "for pair in combos:\n",
    "    if pair == combos[0]:\n",
    "        new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "        dickey_fuller_df = pd.DataFrame(new_series)\n",
    "    else:\n",
    "        new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "        dickey_fuller_df = pd.concat([dickey_fuller_df, new_series], axis=1)\n",
    "        \n",
    "#naming columns\n",
    "dickey_fuller_df.columns = [pair[0] + \"_\" + pair[1] for pair in combos]\n",
    "\n",
    "dickey_fuller_df['const'] = 1\n",
    "\n",
    "dickey_fuller_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>coef</th>\n",
       "      <th>critical_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h13478734_h17247906</td>\n",
       "      <td>-0.142400</td>\n",
       "      <td>-1.812171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h13478734_h19148736</td>\n",
       "      <td>-0.100319</td>\n",
       "      <td>-2.388007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h13478734_h19252818</td>\n",
       "      <td>-0.087050</td>\n",
       "      <td>-2.050789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h13478734_h19641188</td>\n",
       "      <td>-0.129434</td>\n",
       "      <td>-1.496938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h13478734_h22121327</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>0.174070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>h13478734_h24872</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.072271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h13478734_h26698821</td>\n",
       "      <td>-0.220010</td>\n",
       "      <td>-3.662027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h13478734_h8319283</td>\n",
       "      <td>-0.170112</td>\n",
       "      <td>-1.723562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>h13478734_h8536472</td>\n",
       "      <td>-0.228423</td>\n",
       "      <td>-1.649025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>h13478734_h9659886</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.174761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>h17247906_h19148736</td>\n",
       "      <td>-0.136607</td>\n",
       "      <td>-2.957379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h17247906_h19252818</td>\n",
       "      <td>-0.133493</td>\n",
       "      <td>-3.013498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h17247906_h19641188</td>\n",
       "      <td>-0.160670</td>\n",
       "      <td>-2.843671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>h17247906_h22121327</td>\n",
       "      <td>-0.128271</td>\n",
       "      <td>-2.573837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h17247906_h24872</td>\n",
       "      <td>-0.143410</td>\n",
       "      <td>-1.774923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>h17247906_h26698821</td>\n",
       "      <td>-0.161416</td>\n",
       "      <td>-3.282346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h17247906_h8319283</td>\n",
       "      <td>-0.143839</td>\n",
       "      <td>-1.959041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>h17247906_h8536472</td>\n",
       "      <td>-0.190081</td>\n",
       "      <td>-1.712541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>h17247906_h9659886</td>\n",
       "      <td>-0.078570</td>\n",
       "      <td>-2.158734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>h19148736_h19252818</td>\n",
       "      <td>-0.183242</td>\n",
       "      <td>-1.609116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>h19148736_h19641188</td>\n",
       "      <td>-0.149588</td>\n",
       "      <td>-1.848587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>h19148736_h22121327</td>\n",
       "      <td>-0.117630</td>\n",
       "      <td>-0.950730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>h19148736_h24872</td>\n",
       "      <td>-0.076545</td>\n",
       "      <td>-1.017064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>h19148736_h26698821</td>\n",
       "      <td>-0.178992</td>\n",
       "      <td>-1.493047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>h19148736_h8319283</td>\n",
       "      <td>-0.197089</td>\n",
       "      <td>-1.809203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>h19148736_h8536472</td>\n",
       "      <td>-0.173298</td>\n",
       "      <td>-3.287428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>h19148736_h9659886</td>\n",
       "      <td>-0.008189</td>\n",
       "      <td>-0.097587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>h19252818_h19641188</td>\n",
       "      <td>-0.159242</td>\n",
       "      <td>-1.930831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>h19252818_h22121327</td>\n",
       "      <td>-0.072703</td>\n",
       "      <td>-0.843479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>h19252818_h24872</td>\n",
       "      <td>-0.073915</td>\n",
       "      <td>-1.039764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>h19252818_h26698821</td>\n",
       "      <td>-0.215713</td>\n",
       "      <td>-1.547625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>h19252818_h8319283</td>\n",
       "      <td>-0.298576</td>\n",
       "      <td>-2.225154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>h19252818_h8536472</td>\n",
       "      <td>-0.174643</td>\n",
       "      <td>-3.485519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>h19252818_h9659886</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.007793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>h19641188_h22121327</td>\n",
       "      <td>-0.054686</td>\n",
       "      <td>-0.558020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>h19641188_h24872</td>\n",
       "      <td>-0.075843</td>\n",
       "      <td>-0.822969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>h19641188_h26698821</td>\n",
       "      <td>-0.221873</td>\n",
       "      <td>-2.429506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>h19641188_h8319283</td>\n",
       "      <td>-0.243519</td>\n",
       "      <td>-1.906828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>h19641188_h8536472</td>\n",
       "      <td>-0.253708</td>\n",
       "      <td>-3.090969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>h19641188_h9659886</td>\n",
       "      <td>-0.004390</td>\n",
       "      <td>-0.071174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>h22121327_h24872</td>\n",
       "      <td>-0.105810</td>\n",
       "      <td>-1.295737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>h22121327_h26698821</td>\n",
       "      <td>-0.013528</td>\n",
       "      <td>-0.134464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>h22121327_h8319283</td>\n",
       "      <td>-0.121337</td>\n",
       "      <td>-1.109740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>h22121327_h8536472</td>\n",
       "      <td>-0.158644</td>\n",
       "      <td>-3.078321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>h22121327_h9659886</td>\n",
       "      <td>-0.076898</td>\n",
       "      <td>-0.905136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>h24872_h26698821</td>\n",
       "      <td>-0.078528</td>\n",
       "      <td>-0.938257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>h24872_h8319283</td>\n",
       "      <td>-0.061348</td>\n",
       "      <td>-0.754138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>h24872_h8536472</td>\n",
       "      <td>-0.157667</td>\n",
       "      <td>-1.686995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>h24872_h9659886</td>\n",
       "      <td>-0.097598</td>\n",
       "      <td>-1.431097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>h26698821_h8319283</td>\n",
       "      <td>-0.303791</td>\n",
       "      <td>-2.351172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>h26698821_h8536472</td>\n",
       "      <td>-0.212864</td>\n",
       "      <td>-3.514689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>h26698821_h9659886</td>\n",
       "      <td>0.020620</td>\n",
       "      <td>0.292748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>h8319283_h8536472</td>\n",
       "      <td>-0.205399</td>\n",
       "      <td>-2.167536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>h8319283_h9659886</td>\n",
       "      <td>-0.053159</td>\n",
       "      <td>-0.616254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>h8536472_h9659886</td>\n",
       "      <td>-0.091581</td>\n",
       "      <td>-1.849242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pair      coef  critical_value\n",
       "0   h13478734_h17247906 -0.142400       -1.812171\n",
       "1   h13478734_h19148736 -0.100319       -2.388007\n",
       "2   h13478734_h19252818 -0.087050       -2.050789\n",
       "3   h13478734_h19641188 -0.129434       -1.496938\n",
       "4   h13478734_h22121327  0.008812        0.174070\n",
       "5      h13478734_h24872  0.005747        0.072271\n",
       "6   h13478734_h26698821 -0.220010       -3.662027\n",
       "7    h13478734_h8319283 -0.170112       -1.723562\n",
       "8    h13478734_h8536472 -0.228423       -1.649025\n",
       "9    h13478734_h9659886  0.008148        0.174761\n",
       "10  h17247906_h19148736 -0.136607       -2.957379\n",
       "11  h17247906_h19252818 -0.133493       -3.013498\n",
       "12  h17247906_h19641188 -0.160670       -2.843671\n",
       "13  h17247906_h22121327 -0.128271       -2.573837\n",
       "14     h17247906_h24872 -0.143410       -1.774923\n",
       "15  h17247906_h26698821 -0.161416       -3.282346\n",
       "16   h17247906_h8319283 -0.143839       -1.959041\n",
       "17   h17247906_h8536472 -0.190081       -1.712541\n",
       "18   h17247906_h9659886 -0.078570       -2.158734\n",
       "19  h19148736_h19252818 -0.183242       -1.609116\n",
       "20  h19148736_h19641188 -0.149588       -1.848587\n",
       "21  h19148736_h22121327 -0.117630       -0.950730\n",
       "22     h19148736_h24872 -0.076545       -1.017064\n",
       "23  h19148736_h26698821 -0.178992       -1.493047\n",
       "24   h19148736_h8319283 -0.197089       -1.809203\n",
       "25   h19148736_h8536472 -0.173298       -3.287428\n",
       "26   h19148736_h9659886 -0.008189       -0.097587\n",
       "27  h19252818_h19641188 -0.159242       -1.930831\n",
       "28  h19252818_h22121327 -0.072703       -0.843479\n",
       "29     h19252818_h24872 -0.073915       -1.039764\n",
       "30  h19252818_h26698821 -0.215713       -1.547625\n",
       "31   h19252818_h8319283 -0.298576       -2.225154\n",
       "32   h19252818_h8536472 -0.174643       -3.485519\n",
       "33   h19252818_h9659886  0.000520        0.007793\n",
       "34  h19641188_h22121327 -0.054686       -0.558020\n",
       "35     h19641188_h24872 -0.075843       -0.822969\n",
       "36  h19641188_h26698821 -0.221873       -2.429506\n",
       "37   h19641188_h8319283 -0.243519       -1.906828\n",
       "38   h19641188_h8536472 -0.253708       -3.090969\n",
       "39   h19641188_h9659886 -0.004390       -0.071174\n",
       "40     h22121327_h24872 -0.105810       -1.295737\n",
       "41  h22121327_h26698821 -0.013528       -0.134464\n",
       "42   h22121327_h8319283 -0.121337       -1.109740\n",
       "43   h22121327_h8536472 -0.158644       -3.078321\n",
       "44   h22121327_h9659886 -0.076898       -0.905136\n",
       "45     h24872_h26698821 -0.078528       -0.938257\n",
       "46      h24872_h8319283 -0.061348       -0.754138\n",
       "47      h24872_h8536472 -0.157667       -1.686995\n",
       "48      h24872_h9659886 -0.097598       -1.431097\n",
       "49   h26698821_h8319283 -0.303791       -2.351172\n",
       "50   h26698821_h8536472 -0.212864       -3.514689\n",
       "51   h26698821_h9659886  0.020620        0.292748\n",
       "52    h8319283_h8536472 -0.205399       -2.167536\n",
       "53    h8319283_h9659886 -0.053159       -0.616254\n",
       "54    h8536472_h9659886 -0.091581       -1.849242"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dickey fuller test on each column\n",
    "\n",
    "#regression fit and results in vertical dataframe format (column for pair id, column for dickey fuller test result)\n",
    "dickey_fuller_results = {'pair' : [], 'coef' : [], 'critical_value' : []}\n",
    "\n",
    "for column in dickey_fuller_df:\n",
    "    if column == 'const':\n",
    "        break\n",
    "    reg = sm.OLS(endog = dickey_fuller_df[column].diff(), exog = dickey_fuller_df[['const', column]].shift(1), missing = 'drop')\n",
    "    results = reg.fit()\n",
    "    dickey_fuller_results['pair'].append(column)\n",
    "    dickey_fuller_results['coef'].append(results.params[1])\n",
    "    dickey_fuller_results['critical_value'].append(results.tvalues[1])\n",
    "\n",
    "dickey_fuller_results_df = pd.DataFrame(dickey_fuller_results)\n",
    "\n",
    "dickey_fuller_results_df\n",
    "\n",
    "#compare to -3.58 from the MacKinnon tables for 1% significance level, -2.93 for 5% significance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs found\n",
      "Pair row index: 6 , Pair ids: h13478734_h26698821 , Pair DF test critical value: -3.662027128375918 , Pair theta: -0.22000972730609739\n",
      "Pair average spread: -43.04310344827586 , Pair spread standard deviation: 1.9916577865979523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#any pairs with a critical value less than have the null hypothesis rejected at the 1% significance level\n",
    "if dickey_fuller_results_df['critical_value'].min() < - 3.58:\n",
    "    pairs_df = dickey_fuller_results_df.loc[dickey_fuller_results_df['critical_value'] < - 3.58].copy() #all possible pairs\n",
    "    print(f\"Pairs found\")\n",
    "    pair_index = pairs_df['critical_value'].idxmin() #the most stationary pair \n",
    "    pair_cv = pairs_df['critical_value'].min()\n",
    "    pair_ids = pairs_df['pair'].loc[pair_index]\n",
    "    pair_coef = pairs_df['coef'].loc[pair_index]\n",
    "\n",
    "    horse_a = pair_ids.split(\"_\", 1)[0]\n",
    "    horse_b = pair_ids.split(\"_\", 1)[1]\n",
    "    pair_df = bp_t_df[[horse_a, horse_b]]\n",
    "    pair_df = pd.concat([pair_df, lp_t_df[[horse_a, horse_b]]], axis=1)\n",
    "    pair_df.columns = [horse_a + \"_bp\", horse_b + \"_bp\", horse_a + \"_lp\", horse_b + \"_lp\"]\n",
    "    \n",
    "    #the following are all defined only in terms of BP\n",
    "    pair_df['spread'] = pair_df[horse_a + \"_bp\"] - pair_df[horse_b + \"_bp\"]\n",
    "\n",
    "    pair_spread_sd = np.std(pair_df['spread'][0:29], ddof = 1)\n",
    "    pair_spread_mean = pair_df['spread'][0:29].mean()\n",
    "\n",
    "    pair_df['deviation_2sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > 2 * pair_spread_sd, True, False)\n",
    "    pair_df['deviation_1sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > pair_spread_sd, True, False)\n",
    "\n",
    "    print(\"Pair row index: \" + str(pair_index), \", Pair ids: \" + str(pair_ids), \", Pair DF test critical value: \" + str(pair_cv), \", Pair theta: \" + str(pair_coef))\n",
    "    print(\"Pair average spread: \" + str(pair_spread_mean), \", Pair spread standard deviation: \" + str(pair_spread_sd) + \"\\n\")\n",
    "    \n",
    "else: print(\"No pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h13478734_bp</th>\n",
       "      <th>h26698821_bp</th>\n",
       "      <th>h13478734_lp</th>\n",
       "      <th>h26698821_lp</th>\n",
       "      <th>spread</th>\n",
       "      <th>deviation_2sd</th>\n",
       "      <th>deviation_1sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.58</td>\n",
       "      <td>60.00</td>\n",
       "      <td>12.32</td>\n",
       "      <td>68.07</td>\n",
       "      <td>-48.42</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.00</td>\n",
       "      <td>60.11</td>\n",
       "      <td>12.50</td>\n",
       "      <td>70.00</td>\n",
       "      <td>-48.11</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.00</td>\n",
       "      <td>58.98</td>\n",
       "      <td>12.56</td>\n",
       "      <td>68.75</td>\n",
       "      <td>-46.98</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>12.77</td>\n",
       "      <td>62.95</td>\n",
       "      <td>-43.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>12.61</td>\n",
       "      <td>64.55</td>\n",
       "      <td>-43.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.39</td>\n",
       "      <td>55.00</td>\n",
       "      <td>12.93</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-42.61</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.50</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-42.50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.50</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-42.50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.50</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>64.09</td>\n",
       "      <td>-42.50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.50</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.83</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.33</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>62.27</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.12</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.62</td>\n",
       "      <td>61.02</td>\n",
       "      <td>-41.88</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.03</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.54</td>\n",
       "      <td>60.68</td>\n",
       "      <td>-41.97</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.89</td>\n",
       "      <td>55.00</td>\n",
       "      <td>14.39</td>\n",
       "      <td>60.00</td>\n",
       "      <td>-41.11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>14.50</td>\n",
       "      <td>60.11</td>\n",
       "      <td>-41.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.00</td>\n",
       "      <td>58.64</td>\n",
       "      <td>14.51</td>\n",
       "      <td>63.64</td>\n",
       "      <td>-44.64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14.25</td>\n",
       "      <td>60.00</td>\n",
       "      <td>14.75</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.75</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.39</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.61</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.03</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.47</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.12</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>-45.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14.50</td>\n",
       "      <td>60.68</td>\n",
       "      <td>15.00</td>\n",
       "      <td>65.68</td>\n",
       "      <td>-46.18</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14.42</td>\n",
       "      <td>65.00</td>\n",
       "      <td>14.94</td>\n",
       "      <td>70.00</td>\n",
       "      <td>-50.58</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.00</td>\n",
       "      <td>60.70</td>\n",
       "      <td>14.50</td>\n",
       "      <td>70.00</td>\n",
       "      <td>-46.70</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14.00</td>\n",
       "      <td>64.43</td>\n",
       "      <td>14.69</td>\n",
       "      <td>70.00</td>\n",
       "      <td>-50.43</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>14.44</td>\n",
       "      <td>73.18</td>\n",
       "      <td>14.94</td>\n",
       "      <td>82.27</td>\n",
       "      <td>-58.74</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.50</td>\n",
       "      <td>78.75</td>\n",
       "      <td>15.00</td>\n",
       "      <td>88.52</td>\n",
       "      <td>-64.25</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>14.45</td>\n",
       "      <td>79.89</td>\n",
       "      <td>14.96</td>\n",
       "      <td>90.00</td>\n",
       "      <td>-65.44</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>14.06</td>\n",
       "      <td>71.82</td>\n",
       "      <td>14.78</td>\n",
       "      <td>79.77</td>\n",
       "      <td>-57.76</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>14.94</td>\n",
       "      <td>67.61</td>\n",
       "      <td>15.45</td>\n",
       "      <td>75.00</td>\n",
       "      <td>-52.67</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>15.00</td>\n",
       "      <td>68.98</td>\n",
       "      <td>15.50</td>\n",
       "      <td>75.23</td>\n",
       "      <td>-53.98</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14.09</td>\n",
       "      <td>75.00</td>\n",
       "      <td>14.60</td>\n",
       "      <td>82.16</td>\n",
       "      <td>-60.91</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>13.50</td>\n",
       "      <td>78.60</td>\n",
       "      <td>14.00</td>\n",
       "      <td>85.58</td>\n",
       "      <td>-65.10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13.50</td>\n",
       "      <td>84.55</td>\n",
       "      <td>14.12</td>\n",
       "      <td>90.00</td>\n",
       "      <td>-71.05</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>13.98</td>\n",
       "      <td>92.39</td>\n",
       "      <td>14.50</td>\n",
       "      <td>98.07</td>\n",
       "      <td>-78.41</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>14.44</td>\n",
       "      <td>92.61</td>\n",
       "      <td>14.94</td>\n",
       "      <td>100.45</td>\n",
       "      <td>-78.17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>14.69</td>\n",
       "      <td>123.18</td>\n",
       "      <td>15.20</td>\n",
       "      <td>138.86</td>\n",
       "      <td>-108.49</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>16.59</td>\n",
       "      <td>156.36</td>\n",
       "      <td>17.12</td>\n",
       "      <td>167.27</td>\n",
       "      <td>-139.77</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>17.54</td>\n",
       "      <td>165.68</td>\n",
       "      <td>18.04</td>\n",
       "      <td>179.32</td>\n",
       "      <td>-148.14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>17.62</td>\n",
       "      <td>153.18</td>\n",
       "      <td>18.12</td>\n",
       "      <td>165.45</td>\n",
       "      <td>-135.56</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>18.00</td>\n",
       "      <td>140.89</td>\n",
       "      <td>18.50</td>\n",
       "      <td>154.67</td>\n",
       "      <td>-122.89</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>18.00</td>\n",
       "      <td>132.09</td>\n",
       "      <td>18.50</td>\n",
       "      <td>145.35</td>\n",
       "      <td>-114.09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>18.42</td>\n",
       "      <td>132.27</td>\n",
       "      <td>18.92</td>\n",
       "      <td>146.82</td>\n",
       "      <td>-113.85</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>18.56</td>\n",
       "      <td>124.77</td>\n",
       "      <td>19.06</td>\n",
       "      <td>145.23</td>\n",
       "      <td>-106.21</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    h13478734_bp  h26698821_bp  h13478734_lp  h26698821_lp  spread  \\\n",
       "0          11.58         60.00         12.32         68.07  -48.42   \n",
       "1          12.00         60.11         12.50         70.00  -48.11   \n",
       "2          12.00         58.98         12.56         68.75  -46.98   \n",
       "3          12.00         55.00         12.77         62.95  -43.00   \n",
       "4          12.00         55.00         12.61         64.55  -43.00   \n",
       "5          12.39         55.00         12.93         65.00  -42.61   \n",
       "6          12.50         55.00         13.00         65.00  -42.50   \n",
       "7          12.50         55.00         13.00         65.00  -42.50   \n",
       "8          12.50         55.00         13.00         64.09  -42.50   \n",
       "9          12.50         55.00         13.00         60.00  -42.50   \n",
       "10         12.83         55.00         13.33         60.00  -42.17   \n",
       "11         13.00         55.00         13.50         60.00  -42.00   \n",
       "12         13.00         55.00         13.50         60.00  -42.00   \n",
       "13         13.00         55.00         13.50         60.00  -42.00   \n",
       "14         13.00         55.00         13.50         60.00  -42.00   \n",
       "15         13.00         55.00         13.50         60.00  -42.00   \n",
       "16         13.00         55.00         13.50         62.27  -42.00   \n",
       "17         13.00         55.00         13.50         60.00  -42.00   \n",
       "18         13.00         55.00         13.50         60.00  -42.00   \n",
       "19         13.00         55.00         13.50         60.00  -42.00   \n",
       "20         13.12         55.00         13.62         61.02  -41.88   \n",
       "21         13.03         55.00         13.54         60.68  -41.97   \n",
       "22         13.00         55.00         13.50         60.00  -42.00   \n",
       "23         13.00         55.00         13.50         60.00  -42.00   \n",
       "24         13.89         55.00         14.39         60.00  -41.11   \n",
       "25         14.00         55.00         14.50         60.11  -41.00   \n",
       "26         14.00         58.64         14.51         63.64  -44.64   \n",
       "27         14.25         60.00         14.75         65.00  -45.75   \n",
       "28         14.39         60.00         15.00         65.00  -45.61   \n",
       "29         14.50         60.00         15.03         65.00  -45.50   \n",
       "30         14.50         60.00         15.00         65.00  -45.50   \n",
       "31         14.50         60.00         15.47         65.00  -45.50   \n",
       "32         14.50         60.00         15.12         65.00  -45.50   \n",
       "33         14.50         60.00         15.00         65.00  -45.50   \n",
       "34         14.50         60.00         15.00         65.00  -45.50   \n",
       "35         14.50         60.00         15.00         65.00  -45.50   \n",
       "36         14.50         60.00         15.00         65.00  -45.50   \n",
       "37         14.50         60.68         15.00         65.68  -46.18   \n",
       "38         14.42         65.00         14.94         70.00  -50.58   \n",
       "39         14.00         60.70         14.50         70.00  -46.70   \n",
       "40         14.00         64.43         14.69         70.00  -50.43   \n",
       "41         14.44         73.18         14.94         82.27  -58.74   \n",
       "42         14.50         78.75         15.00         88.52  -64.25   \n",
       "43         14.45         79.89         14.96         90.00  -65.44   \n",
       "44         14.06         71.82         14.78         79.77  -57.76   \n",
       "45         14.94         67.61         15.45         75.00  -52.67   \n",
       "46         15.00         68.98         15.50         75.23  -53.98   \n",
       "47         14.09         75.00         14.60         82.16  -60.91   \n",
       "48         13.50         78.60         14.00         85.58  -65.10   \n",
       "49         13.50         84.55         14.12         90.00  -71.05   \n",
       "50         13.98         92.39         14.50         98.07  -78.41   \n",
       "51         14.44         92.61         14.94        100.45  -78.17   \n",
       "52         14.69        123.18         15.20        138.86 -108.49   \n",
       "53         16.59        156.36         17.12        167.27 -139.77   \n",
       "54         17.54        165.68         18.04        179.32 -148.14   \n",
       "55         17.62        153.18         18.12        165.45 -135.56   \n",
       "56         18.00        140.89         18.50        154.67 -122.89   \n",
       "57         18.00        132.09         18.50        145.35 -114.09   \n",
       "58         18.42        132.27         18.92        146.82 -113.85   \n",
       "59         18.56        124.77         19.06        145.23 -106.21   \n",
       "\n",
       "    deviation_2sd  deviation_1sd  \n",
       "0            True           True  \n",
       "1            True           True  \n",
       "2           False           True  \n",
       "3           False          False  \n",
       "4           False          False  \n",
       "5           False          False  \n",
       "6           False          False  \n",
       "7           False          False  \n",
       "8           False          False  \n",
       "9           False          False  \n",
       "10          False          False  \n",
       "11          False          False  \n",
       "12          False          False  \n",
       "13          False          False  \n",
       "14          False          False  \n",
       "15          False          False  \n",
       "16          False          False  \n",
       "17          False          False  \n",
       "18          False          False  \n",
       "19          False          False  \n",
       "20          False          False  \n",
       "21          False          False  \n",
       "22          False          False  \n",
       "23          False          False  \n",
       "24          False          False  \n",
       "25          False          False  \n",
       "26          False          False  \n",
       "27          False           True  \n",
       "28          False           True  \n",
       "29          False           True  \n",
       "30          False           True  \n",
       "31          False           True  \n",
       "32          False           True  \n",
       "33          False           True  \n",
       "34          False           True  \n",
       "35          False           True  \n",
       "36          False           True  \n",
       "37          False           True  \n",
       "38           True           True  \n",
       "39          False           True  \n",
       "40           True           True  \n",
       "41           True           True  \n",
       "42           True           True  \n",
       "43           True           True  \n",
       "44           True           True  \n",
       "45           True           True  \n",
       "46           True           True  \n",
       "47           True           True  \n",
       "48           True           True  \n",
       "49           True           True  \n",
       "50           True           True  \n",
       "51           True           True  \n",
       "52           True           True  \n",
       "53           True           True  \n",
       "54           True           True  \n",
       "55           True           True  \n",
       "56           True           True  \n",
       "57           True           True  \n",
       "58           True           True  \n",
       "59           True           True  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at repeated versions of the table above, on occasions where the price has drifted more than two standard deviations from the mean, it has returned to the mean after around 5 time periods (10 minutes). I will look at a pairs trading strategy where the pairs trade is made upon deviation and closed 10 minutes after the deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win side = -0.0360962566844929. Loss side = -0.03609625668449201.\n",
      "Win side = 0.12379521842534302. Loss side = 0.12379521842533481.\n"
     ]
    }
   ],
   "source": [
    "k = 5 #number of periods trade is kept open\n",
    " \n",
    "open_trade_df = pair_df[pair_df['deviation_2sd'] == True].loc[30:] #so that we only consider data after the first 30 periods\n",
    "open_trade_idx = open_trade_df.index[0] \n",
    "    \n",
    "close_trade_idx = open_trade_idx + k\n",
    "\n",
    "#to work out what the direction of the trade should be\n",
    "#the spreads are always defined as horse_a - horse_b, so if the spread > average, btl horse_a and ltb horse_b. if below average, vice versa.\n",
    "if (pair_df['spread'].iloc[open_trade_idx] > pair_spread_mean and pair_spread_mean > 0) or (pair_df['spread'].iloc[open_trade_idx] < pair_spread_mean and pair_spread_mean < 0):\n",
    "    #back to lay A\n",
    "    bp_a = pair_df[horse_a + \"_bp\"].iloc[open_trade_idx]\n",
    "    lp_a = pair_df[horse_a + \"_lp\"].iloc[close_trade_idx]\n",
    "    \n",
    "    win_side_a, loss_side_a = payout(bp_a, 1, lp_a, '?')\n",
    "    print(f\"Win side = {win_side_a}. Loss side = {loss_side_a}.\")\n",
    "    \n",
    "    #lay to back X\n",
    "    lp_b = pair_df[horse_b + \"_lp\"].iloc[open_trade_idx]\n",
    "    bp_b = pair_df[horse_b + \"_bp\"].iloc[close_trade_idx]\n",
    "    \n",
    "    win_side_b, loss_side_b = payout(bp_b, '?', lp_b, 1)\n",
    "    print(f\"Win side = {win_side_b}. Loss side = {loss_side_b}.\")\n",
    "    \n",
    "else: \n",
    "    #lay to back Y\n",
    "    lp_a = pair_df[horse_a + \"_lp\"].iloc[open_trade_idx]\n",
    "    bp_a = pair_df[horse_a + \"_bp\"].iloc[close_trade_idx]\n",
    "    \n",
    "    win_side_a, loss_side_a = payout(bp_a, '?', lp_a, 1)\n",
    "    print(f\"Win side = {win_side_a}. Loss side = {loss_side_a}.\")\n",
    "    \n",
    "    #back to lay X\n",
    "    bp_b = pair_df[horse_b + \"_bp\"].iloc[open_trade_idx]\n",
    "    lp_b = pair_df[horse_b + \"_lp\"].iloc[close_trade_idx]\n",
    "    \n",
    "    win_side_b, loss_side_b = payout(bp_b, 1, lp_b, '?')\n",
    "    print(f\"Win side = {win_side_b}. Loss side = {loss_side_b}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo simulation**\n",
    "\n",
    "n repetitions of the above with profit summed over all trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 1000 iterations completed.\n",
      "200 of 1000 iterations completed.\n",
      "300 of 1000 iterations completed.\n",
      "400 of 1000 iterations completed.\n",
      "500 of 1000 iterations completed.\n",
      "600 of 1000 iterations completed.\n",
      "700 of 1000 iterations completed.\n",
      "800 of 1000 iterations completed.\n",
      "900 of 1000 iterations completed.\n",
      "1000 of 1000 iterations completed.\n",
      "Profit over 1000 iterations = -32.48763332543176. 408 pairs found and 310 pairs trades made.\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "n = 1000 #number of iterations\n",
    "k = 5 #number of 2 mintue periods trade is kept open (i.e. time expected for mean reversion to occur)\n",
    "\n",
    "#results variables\n",
    "profit = 0\n",
    "num_pairs = 0\n",
    "num_trades = 0\n",
    "\n",
    "for i in range(n):\n",
    "            \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"{i+1} of {n} iterations completed.\")\n",
    "    \n",
    "    #new sample\n",
    "    sample_df = df[df['MarketId'] == df['MarketId'].sample(1).item()]\n",
    "    sample_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    bp_df = sample_df[['SelectionId'] + back_prices].copy()\n",
    "    new_cols = bp_df.columns.str.replace(\"[BP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "    bp_df.rename(columns = dict(zip(bp_df.columns, new_cols)), inplace = True)\n",
    "    bp_t_df = bp_df.T.copy()\n",
    "    bp_t_df.columns = [\"h\" + str(int(column)) for column in bp_t_df.iloc[0]]\n",
    "    bp_t_df = bp_t_df.iloc[1:-15] # using the 60 pre-off price data points\n",
    "    bp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    lp_df = sample_df[['SelectionId'] + lay_prices].copy()\n",
    "    new_cols = lp_df.columns.str.replace(\"[LP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "    lp_df.rename(columns = dict(zip(lp_df.columns, new_cols)), inplace = True)\n",
    "    lp_t_df = lp_df.T.copy()\n",
    "    lp_t_df.columns = [\"h\" + str(int(column)) for column in lp_t_df.iloc[0]] #rename columns to horse ids\n",
    "    lp_t_df = lp_t_df.iloc[1:-15] #remove horse ids, remove inplay data\n",
    "    lp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #using non-standardised log price data\n",
    "    log_bp = np.log(bp_t_df[:30]).copy()\n",
    "    log_bp.head()\n",
    "\n",
    "    #create dataframe where each column is log(horse a's prices) - log(horse b's prices). one new column for all n(n-1)/2 possible pairs\n",
    "    #use itertools to find all possible comination pairs\n",
    "    combos = list(itertools.combinations(log_bp.columns, 2))\n",
    "\n",
    "    for pair in combos:\n",
    "        if pair == combos[0]:\n",
    "            new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "            dickey_fuller_df = pd.DataFrame(new_series)\n",
    "        else:\n",
    "            new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "            dickey_fuller_df = pd.concat([dickey_fuller_df, new_series], axis=1)\n",
    "\n",
    "    #naming columns\n",
    "    dickey_fuller_df.columns = [pair[0] + \"_\" + pair[1] for pair in combos]\n",
    "    dickey_fuller_df['const'] = 1\n",
    "\n",
    "    #dickey fuller test on each column\n",
    "    dickey_fuller_results = {'pair' : [], 'coef' : [], 'critical_value' : []}\n",
    "\n",
    "    for column in dickey_fuller_df:\n",
    "        if column == 'const':\n",
    "            continue\n",
    "        reg = sm.OLS(endog = dickey_fuller_df[column].diff(), exog = dickey_fuller_df[['const', column]].shift(1), missing = 'drop')\n",
    "        results = reg.fit()\n",
    "        dickey_fuller_results['pair'].append(column)\n",
    "        dickey_fuller_results['coef'].append(results.params[1])\n",
    "        dickey_fuller_results['critical_value'].append(results.tvalues[1])\n",
    "\n",
    "    dickey_fuller_results_df = pd.DataFrame(dickey_fuller_results)\n",
    "\n",
    "    #any pairs with a critical value less than have the null hypothesis rejected at the 1% significance level\n",
    "    if dickey_fuller_results_df['critical_value'].min() < - 3.58:\n",
    "        pairs_df = dickey_fuller_results_df.loc[dickey_fuller_results_df['critical_value'] < - 3.58].copy()\n",
    "        \n",
    "        num_pairs += 1\n",
    "\n",
    "        pair_index = pairs_df['critical_value'].idxmin()\n",
    "        pair_cv = pairs_df['critical_value'].min()\n",
    "        pair_ids = pairs_df['pair'].loc[pair_index]\n",
    "        pair_coef = pairs_df['coef'].loc[pair_index]\n",
    "\n",
    "        horse_a = pair_ids.split(\"_\", 1)[0]\n",
    "        horse_b = pair_ids.split(\"_\", 1)[1]\n",
    "        pair_df = bp_t_df[[horse_a, horse_b]]\n",
    "        pair_df = pd.concat([pair_df, lp_t_df[[horse_a, horse_b]]], axis=1)\n",
    "        pair_df.columns = [horse_a + \"_bp\", horse_b + \"_bp\", horse_a + \"_lp\", horse_b + \"_lp\"]\n",
    "\n",
    "        #the following are all defined only in terms of BP\n",
    "        pair_df['spread'] = pair_df[horse_a + \"_bp\"] - pair_df[horse_b + \"_bp\"]\n",
    "\n",
    "        pair_spread_sd = np.std(pair_df['spread'][0:29], ddof = 1)\n",
    "        pair_spread_mean = pair_df['spread'][0:29].mean()\n",
    "\n",
    "        pair_df['deviation_2sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > 2 * pair_spread_sd, True, False)\n",
    "        pair_df['deviation_1sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > pair_spread_sd, True, False)\n",
    "\n",
    "    else: continue #move on to next iteration if there are no stationary series\n",
    "\n",
    "    #profit calculation  \n",
    "\n",
    "    open_trade_df = pair_df[pair_df['deviation_2sd'] == True].loc[30:] #so that we only consider data after the first 30 periods\n",
    "    if len(open_trade_df.index) == 0:\n",
    "        continue #move on to next iteration if there are no trading opportunities\n",
    "        \n",
    "    num_trades += 1\n",
    "    \n",
    "    open_trade_idx = open_trade_df.index[0]\n",
    "    close_trade_idx = open_trade_idx + k\n",
    "    \n",
    "    if close_trade_idx > 59: \n",
    "        continue #move on to next iteration if trade cant be simulated\n",
    "\n",
    "    #to work out what the direction of the trade should be\n",
    "    #the spreads are always defined as horse_a - horse_b, so if the spread > average, btl horse_a and ltb horse_b. if below average, vice versa.\n",
    "    \n",
    "    if (pair_df['spread'].iloc[open_trade_idx] > pair_spread_mean and pair_spread_mean > 0) or (pair_df['spread'].iloc[open_trade_idx] < pair_spread_mean and pair_spread_mean < 0):\n",
    "        #back to lay A\n",
    "        bp_a = pair_df[horse_a + \"_bp\"].iloc[open_trade_idx]\n",
    "        lp_a = pair_df[horse_a + \"_lp\"].iloc[close_trade_idx]\n",
    "\n",
    "        win_side_a, loss_side_a = payout(bp_a, 1, lp_a, '?')\n",
    "\n",
    "        #lay to back X\n",
    "        lp_b = pair_df[horse_b + \"_lp\"].iloc[open_trade_idx]\n",
    "        bp_b = pair_df[horse_b + \"_bp\"].iloc[close_trade_idx]\n",
    "\n",
    "        win_side_b, loss_side_b = payout(bp_b, '?', lp_b, 1)\n",
    "\n",
    "        profit += win_side_a + win_side_b\n",
    "        \n",
    "    else: \n",
    "        #lay to back Y\n",
    "        lp_a = pair_df[horse_a + \"_lp\"].iloc[open_trade_idx]\n",
    "        bp_a = pair_df[horse_a + \"_bp\"].iloc[close_trade_idx]\n",
    "\n",
    "        win_side_a, loss_side_a = payout(bp_a, '?', lp_a, 1)\n",
    "        \n",
    "        #back to lay X\n",
    "        bp_b = pair_df[horse_b + \"_bp\"].iloc[open_trade_idx]\n",
    "        lp_b = pair_df[horse_b + \"_lp\"].iloc[close_trade_idx]\n",
    "\n",
    "        win_side_b, loss_side_b = payout(bp_b, 1, lp_b, '?')\n",
    "\n",
    "        profit += win_side_a + win_side_b\n",
    "        \n",
    "print(f\"Profit over {n} iterations = {profit}. {num_pairs} pairs found and {num_trades} pairs trades made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
